{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Transformer Sentence Embedding\"\n",
        "format:\n",
        "  html:\n",
        "    code-fold: true\n",
        "jupyter: python3\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from transformers import AutoModel, AutoTokenizer, pipeline\n",
        "import torch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model_name = 'sentence-transformers/all-MiniLM-L6-v2'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModel.from_pretrained(model_name)\n",
        "\n",
        "text = \"today I solved some excellent algorithmic problems\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## List All Tokens"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "encoded = tokenizer(text, return_tensors='pt', padding=True, truncation=True)\n",
        "\n",
        "tokens = encoded.input_ids.tolist()[0]\n",
        "\n",
        "for i, token_id in enumerate(tokens):\n",
        "    decoded_token = tokenizer.decode(token_id)\n",
        "    print(f\"Token {i + 1}: {decoded_token}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Token IDs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(encoded['input_ids'])\n",
        "print(encoded['attention_mask'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Executing the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "output = model(**encoded)\n",
        "print(output.last_hidden_state.shape)\n",
        "print(output.last_hidden_state)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Mean Pooling (Manually)\n",
        "\n",
        "Since the attention mask consists entirely of `1`s (no padding tokens),\n",
        "we can safely compute the simple mean over all token embeddings.\n",
        "If the attention mask contains `0`s (indicating padding),\n",
        "a weighted mean that accounts for valid tokens is required instead."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "assert torch.all(encoded['attention_mask'] == 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "manual_pooling = output.last_hidden_state.mean(dim=0)\n",
        "print(manual_pooling)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Calculate Pooling Using Library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "extractor = pipeline(\"feature-extraction\", model=model_name)\n",
        "pooling = torch.tensor(extractor(text))\n",
        "print(pooling)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Compare Both Values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "torch.allclose(manual_pooling, pooling, rtol=1e-5, atol=1e-8)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/usr/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}