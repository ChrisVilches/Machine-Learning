{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Transformer Sentence Embedding"
      ],
      "id": "b433a979-558d-4c9b-b0c9-243df2b78307"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoModel, AutoTokenizer\n",
        "import torch\n",
        "torch.set_printoptions(sci_mode=False)"
      ],
      "id": "d6668f56"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_name = 'sentence-transformers/all-MiniLM-L6-v2'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModel.from_pretrained(model_name)\n",
        "\n",
        "text = \"today I solved some excellent algorithmic problems\""
      ],
      "id": "a979bce0"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## List All Tokens"
      ],
      "id": "fba8667f-7d3e-4a1d-bad9-83b42c2be04d"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token 1: [CLS]\n",
            "Token 2: today\n",
            "Token 3: i\n",
            "Token 4: solved\n",
            "Token 5: some\n",
            "Token 6: excellent\n",
            "Token 7: algorithm\n",
            "Token 8: ##ic\n",
            "Token 9: problems\n",
            "Token 10: [SEP]"
          ]
        }
      ],
      "source": [
        "encoded = tokenizer(text, return_tensors='pt', padding=True, truncation=True)\n",
        "\n",
        "tokens = encoded.input_ids.tolist()[0]\n",
        "\n",
        "for i, token_id in enumerate(tokens):\n",
        "    decoded_token = tokenizer.decode(token_id)\n",
        "    print(f\"Token {i + 1}: {decoded_token}\")"
      ],
      "id": "c7f26c8a"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Token IDs"
      ],
      "id": "e5843e95-c2e1-4726-bd60-14e42056b5b0"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[  101,  2651,  1045, 13332,  2070,  6581,  9896,  2594,  3471,   102]])\n",
            "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])"
          ]
        }
      ],
      "source": [
        "print(encoded['input_ids'])\n",
        "print(encoded['attention_mask'])"
      ],
      "id": "8e48da14"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Initial Token Embeddings (Pre-Transformer)"
      ],
      "id": "5dbb823f-e362-48ae-8377-88eaa1a7ca25"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 10, 384])\n",
            "tensor([[[-0.1766, -0.0482,  0.0377,  ...,  0.0310,  0.1154, -0.2001],\n",
            "         [ 0.1899,  0.4778, -0.6466,  ...,  0.0499, -0.5361,  0.4469],\n",
            "         [-0.2715, -0.5152,  0.1800,  ..., -0.5518, -0.7233, -0.7180],\n",
            "         ...,\n",
            "         [-0.7215, -0.0884,  0.2669,  ...,  0.9165, -0.4877, -0.6912],\n",
            "         [-0.5559, -0.1996, -0.2737,  ...,  0.8535, -0.2942,  0.7360],\n",
            "         [ 0.2826,  0.1163, -0.2290,  ...,  0.1418, -0.0491, -0.1000]]],\n",
            "       grad_fn=<NativeLayerNormBackward0>)"
          ]
        }
      ],
      "source": [
        "initial_embeddings = model.embeddings(encoded.input_ids)\n",
        "print(initial_embeddings.shape)\n",
        "print(initial_embeddings)"
      ],
      "id": "52339ced"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Processing Embeddings with the Transformer"
      ],
      "id": "a146f3c7-31c4-4503-b313-dc5589224e28"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 10, 384])\n",
            "tensor([[[-0.1642,  0.4660,  0.0489,  ...,  0.2386, -0.0663, -0.0930],\n",
            "         [-0.5260,  0.8277,  0.7240,  ..., -0.7451, -1.1621,  1.1199],\n",
            "         [ 0.1473,  0.2505,  0.1579,  ..., -0.2087, -1.1386, -0.6368],\n",
            "         ...,\n",
            "         [-0.5978,  0.6489, -0.2759,  ...,  0.2102, -0.1217, -0.2887],\n",
            "         [-0.8411,  0.7750,  0.2105,  ...,  1.0205, -0.1044,  0.5411],\n",
            "         [-0.3763,  0.5162, -0.3255,  ...,  0.2374, -0.0547, -0.1911]]],\n",
            "       grad_fn=<NativeLayerNormBackward0>)"
          ]
        }
      ],
      "source": [
        "output = model(**encoded)\n",
        "print(output.last_hidden_state.shape)\n",
        "print(output.last_hidden_state)"
      ],
      "id": "6efb90ea"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Mean Pooling\n",
        "\n",
        "Since the attention mask consists entirely of `1`s (no padding tokens),\n",
        "we can safely compute the simple mean over all token embeddings. If the\n",
        "attention mask contains `0`s (indicating padding), a weighted mean that\n",
        "accounts for valid tokens is required instead."
      ],
      "id": "47a334fd-7f67-49d5-8ece-77eb28465dac"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "assert torch.all(encoded['attention_mask'] == 1)"
      ],
      "id": "1ecc82e1"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[    -0.4843,      0.5799,      0.1977,     -0.4021,     -0.1579,\n",
            "             -0.6034,      0.2125,     -0.1393,     -0.8927,      0.0554,\n",
            "             -0.2813,      0.2256,      0.2001,      0.6244,     -0.1812,\n",
            "              0.4415,     -0.5326,      0.3251,     -0.2603,     -0.4146,\n",
            "             -0.8319,      0.1059,     -0.3719,      0.0210,     -0.0178,\n",
            "              0.4471,      0.1659,     -0.6292,      0.3771,     -0.2244,\n",
            "             -0.1410,      0.2364,      0.2751,      0.0318,     -0.1013,\n",
            "              0.1692,     -0.1336,     -0.0603,     -0.1009,      0.1103,\n",
            "             -0.1001,     -0.0398,      0.0316,     -0.1058,      0.1335,\n",
            "             -0.0435,     -0.2680,      0.2892,      0.2959,     -0.4699,\n",
            "             -0.6845,     -0.3565,     -0.4861,     -0.3331,     -0.1521,\n",
            "             -0.5225,      0.3661,     -0.0263,      0.2539,     -0.3906,\n",
            "              0.2157,     -0.1258,     -0.2334,      0.0285,      0.3230,\n",
            "              0.2593,      0.0283,     -0.5552,     -0.1394,      0.6151,\n",
            "             -0.2140,      0.4829,     -0.1237,      0.4345,      0.4715,\n",
            "              0.4437,      0.2531,     -0.1291,     -0.2473,     -0.0693,\n",
            "              0.2651,     -0.5477,     -0.3068,      0.1395,      0.2763,\n",
            "             -0.3864,     -0.4963,      0.2176,     -0.0005,     -0.3579,\n",
            "              0.3098,     -0.1587,     -0.1121,      0.0391,      0.4096,\n",
            "             -0.2712,     -0.0501,      0.1397,     -0.2059,      0.6566,\n",
            "             -0.2621,      0.0332,     -0.0540,     -0.1450,      0.1727,\n",
            "              0.4734,      0.0376,      0.2463,      0.1786,     -0.6267,\n",
            "             -0.1425,      0.1571,      0.2398,      0.1425,      0.1867,\n",
            "             -0.1041,     -0.0045,      0.2846,     -0.1385,      0.1005,\n",
            "             -0.0509,      0.0079,      0.0923,      0.0471,      0.0649,\n",
            "              0.1063,     -0.0334,     -0.0000,     -0.3601,      0.0898,\n",
            "              0.5409,     -0.1702,      0.1390,     -0.2456,     -0.1474,\n",
            "              0.0324,     -0.0326,     -0.3734,      0.3124,      0.2060,\n",
            "              0.1401,     -0.0036,      0.4541,     -0.3631,      0.1023,\n",
            "              0.1812,     -0.1487,     -0.1502,      0.2924,     -0.4913,\n",
            "              0.3597,     -0.1143,      0.3122,      0.3770,      0.3220,\n",
            "             -0.8939,      0.5624,      0.0725,     -0.1044,      0.0510,\n",
            "             -0.2841,      0.7995,      0.0680,      0.4625,      0.0365,\n",
            "             -0.3607,      0.1558,     -0.1776,      0.0356,      0.1384,\n",
            "              0.4099,     -0.1688,      0.3094,     -0.1007,     -0.1441,\n",
            "              0.2971,      0.3767,      0.1585,     -0.6690,      0.1080,\n",
            "             -0.1782,     -0.2233,     -0.0879,     -0.0720,     -0.3389,\n",
            "             -0.0238,      0.2306,      0.6153,      0.1001,     -0.0870,\n",
            "              0.0225,     -0.5089,     -0.4442,      0.1925,     -0.2786,\n",
            "             -0.2974,      0.4679,      0.6808,     -0.3641,      0.1131,\n",
            "              0.4033,      0.1229,      0.2214,      0.1550,      0.1972,\n",
            "             -0.4730,     -0.2536,     -0.3027,     -0.0091,      0.3262,\n",
            "              0.1064,     -0.6588,      0.4005,     -0.3173,     -0.1350,\n",
            "              0.0349,     -0.2483,     -0.0919,     -0.4166,      0.3041,\n",
            "              0.2201,      0.1230,      0.2082,      0.0000,     -0.4009,\n",
            "             -0.1329,     -0.1993,      0.6950,     -0.1686,     -0.4731,\n",
            "             -0.4041,     -0.3206,      0.1366,      0.3633,      0.3597,\n",
            "              0.2542,     -0.1374,      0.0797,     -0.0375,     -0.3714,\n",
            "             -0.2473,     -0.3142,     -0.0388,      0.0961,     -0.4267,\n",
            "              0.7661,     -0.3403,     -0.1898,      0.2572,      0.2646,\n",
            "              0.2331,     -0.0056,     -0.0427,      0.3761,     -0.1061,\n",
            "             -0.0987,     -0.6711,      0.0075,     -0.0907,      0.1675,\n",
            "              0.2999,     -0.3291,      0.2854,     -0.2828,     -0.0547,\n",
            "              0.0974,     -0.1821,      0.0422,      0.2298,     -0.0493,\n",
            "             -0.3056,      0.0776,      0.0539,      0.1264,     -0.1456,\n",
            "             -0.0523,      0.1926,     -0.4181,      0.1658,      0.2727,\n",
            "              0.0092,      0.1312,      0.2782,      0.1964,     -0.7067,\n",
            "              0.2417,      0.0434,      0.0884,      0.2605,     -0.4012,\n",
            "             -0.0073,     -0.0111,     -0.3445,     -0.1416,      0.0709,\n",
            "              0.2578,     -0.2741,      0.3758,     -0.3986,      0.2190,\n",
            "              0.1871,      0.2243,     -0.0078,     -0.0713,      0.1776,\n",
            "              0.1507,      0.1399,     -0.2272,     -0.2635,     -0.0985,\n",
            "              0.6314,     -0.4100,      0.0443,      0.0470,     -0.2344,\n",
            "              0.1270,      0.1976,     -0.0542,      0.0158,     -0.0000,\n",
            "             -0.0132,     -0.1402,      0.0744,      0.1821,      0.4694,\n",
            "             -0.1186,     -0.2848,      0.4985,     -0.6467,     -0.4128,\n",
            "              0.4792,      0.1128,     -0.0689,      0.6393,      0.2527,\n",
            "             -0.3986,      0.3989,     -0.1859,     -0.3546,     -0.4153,\n",
            "              0.3245,      0.1941,      0.1212,      0.1473,     -0.2531,\n",
            "              0.1930,      0.2836,     -0.0462,     -0.0162,      0.4370,\n",
            "              0.0426,      0.2306,      0.0359,     -0.0838,      0.3576,\n",
            "              0.2539,     -0.2389,     -0.0139,     -0.4698,     -0.6890,\n",
            "              0.0202,      0.2420,     -0.0063,     -0.1899,      0.5224,\n",
            "              0.0539,     -0.0687,      0.0339,     -0.2234,      0.1866,\n",
            "             -0.1949,      0.2493,      0.0531,      0.1099,      0.4431,\n",
            "             -0.2075,      0.1338,     -0.4599,      0.0789,      0.2508,\n",
            "              0.1400,      0.2352,     -0.1897,      0.0249]],\n",
            "       grad_fn=<MeanBackward1>)"
          ]
        }
      ],
      "source": [
        "pooling = output.last_hidden_state.mean(dim=1)\n",
        "print(pooling)"
      ],
      "id": "8a2c2bec"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "path": "/usr/share/jupyter/kernels/python3"
    },
    "language_info": {
      "name": "python",
      "codemirror_mode": {
        "name": "ipython",
        "version": "3"
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  }
}