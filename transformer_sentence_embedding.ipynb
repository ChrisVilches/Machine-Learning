{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Transformer Sentence Embedding"
      ],
      "id": "791abc9e-2820-4095-9a75-8258a084f28b"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoModel, AutoTokenizer\n",
        "import torch\n",
        "torch.set_printoptions(sci_mode=False, edgeitems=10)"
      ],
      "id": "f2b0a4e5"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_name = 'sentence-transformers/all-MiniLM-L6-v2'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModel.from_pretrained(model_name)\n",
        "\n",
        "text = \"today I solved some excellent algorithmic problems\""
      ],
      "id": "f062d58f"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## List All Tokens"
      ],
      "id": "235bab36-5098-4b94-b895-53f251cfc9ba"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token 1: [CLS]\n",
            "Token 2: today\n",
            "Token 3: i\n",
            "Token 4: solved\n",
            "Token 5: some\n",
            "Token 6: excellent\n",
            "Token 7: algorithm\n",
            "Token 8: ##ic\n",
            "Token 9: problems\n",
            "Token 10: [SEP]"
          ]
        }
      ],
      "source": [
        "encoded = tokenizer(text, return_tensors='pt', padding=True, truncation=True)\n",
        "\n",
        "tokens = encoded.input_ids.tolist()[0]\n",
        "\n",
        "for i, token_id in enumerate(tokens):\n",
        "    decoded_token = tokenizer.decode(token_id)\n",
        "    print(f\"Token {i + 1}: {decoded_token}\")"
      ],
      "id": "e6b79bf4"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Token IDs"
      ],
      "id": "c7ea18cb-967c-4e5d-863f-31237ad7a5a7"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[  101,  2651,  1045, 13332,  2070,  6581,  9896,  2594,  3471,   102]])\n",
            "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])"
          ]
        }
      ],
      "source": [
        "print(encoded['input_ids'])\n",
        "print(encoded['attention_mask'])"
      ],
      "id": "55086922"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Initial Token Embeddings (Pre-Transformer)"
      ],
      "id": "4fb6529b-9955-4cf4-b5f2-524050f4eb5e"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 10, 384])\n",
            "tensor([[[    -0.1766,     -0.0482,      0.0377,     -0.0157,      0.0063,\n",
            "              -0.0312,     -0.0682,     -0.0068,      0.0010,      0.1356,\n",
            "           ...,      0.0268,     -0.0989,     -0.0329,     -0.1181,\n",
            "              -0.0277,      0.0379,      0.1696,      0.0310,      0.1154,\n",
            "              -0.2001],\n",
            "         [     0.1899,      0.4778,     -0.6466,     -0.0620,      0.4477,\n",
            "              -0.2278,      0.5765,      0.1068,      0.0290,      0.1077,\n",
            "           ...,      0.0714,      0.0220,     -0.2712,      0.4672,\n",
            "               0.0867,     -0.3027,     -0.8269,      0.0499,     -0.5361,\n",
            "               0.4469],\n",
            "         [    -0.2715,     -0.5152,      0.1800,      0.1759,      0.1213,\n",
            "               0.6639,      1.2929,      0.4178,     -0.2799,      0.3029,\n",
            "           ...,      0.6342,      0.6704,     -0.0137,      0.5679,\n",
            "               0.1162,     -0.0806,      1.1190,     -0.5518,     -0.7233,\n",
            "              -0.7180],\n",
            "         [    -0.6097,     -0.4077,     -0.3073,      0.0387,     -0.2921,\n",
            "              -0.0420,     -0.5815,      0.9135,     -0.4131,     -0.1617,\n",
            "           ...,      0.8308,     -1.4773,      0.2444,     -0.0795,\n",
            "               0.6548,     -0.1415,     -0.0782,     -0.0844,      0.1180,\n",
            "               0.1815],\n",
            "         [    -0.1910,     -0.3607,      0.8348,      0.0936,      0.3426,\n",
            "               1.0948,      0.8128,     -0.4641,      0.6479,     -0.2279,\n",
            "           ...,      0.8264,     -0.5240,     -0.2922,     -0.6893,\n",
            "               0.6151,     -0.2924,     -1.2255,      0.4598,      0.1018,\n",
            "               1.4967],\n",
            "         [    -0.0791,      0.8314,      0.6794,      0.6199,      0.1778,\n",
            "              -0.2140,     -0.9155,     -1.1262,      0.0235,     -0.4903,\n",
            "           ...,      0.3988,     -0.9593,     -0.2144,     -0.5251,\n",
            "              -0.6548,     -0.2922,      0.6357,      1.1416,     -0.3689,\n",
            "              -0.1879],\n",
            "         [    -0.6089,     -0.2179,      0.0612,      0.7646,     -0.4000,\n",
            "               0.2701,     -0.5252,      0.5138,      0.2209,     -0.5732,\n",
            "           ...,     -0.2655,     -0.1617,     -0.2107,     -1.0361,\n",
            "              -0.6808,      0.7202,     -0.5597,      0.1309,      0.6490,\n",
            "              -0.4538],\n",
            "         [    -0.7215,     -0.0884,      0.2669,     -0.2393,     -0.1560,\n",
            "              -0.3931,      0.0353,     -0.4814,     -0.2945,     -0.1962,\n",
            "           ...,     -0.1018,      0.5991,     -0.1734,      1.5785,\n",
            "              -1.2094,      0.0523,     -0.2557,      0.9165,     -0.4877,\n",
            "              -0.6912],\n",
            "         [    -0.5559,     -0.1996,     -0.2737,      0.4928,     -0.2631,\n",
            "              -0.0915,      0.3388,      0.8302,     -0.0578,     -1.0963,\n",
            "           ...,      0.8883,     -0.6742,     -0.8279,     -0.3266,\n",
            "               0.1297,     -0.4326,     -0.2903,      0.8535,     -0.2942,\n",
            "               0.7360],\n",
            "         [     0.2826,      0.1163,     -0.2290,      0.0818,      0.1552,\n",
            "               0.1993,      0.3692,     -0.0015,     -0.4129,     -0.2242,\n",
            "           ...,      0.3181,      0.6487,     -0.5582,      0.3803,\n",
            "              -0.2637,      0.0954,     -0.1195,      0.1418,     -0.0491,\n",
            "              -0.1000]]], grad_fn=<NativeLayerNormBackward0>)"
          ]
        }
      ],
      "source": [
        "initial_embeddings = model.embeddings(encoded.input_ids)\n",
        "print(initial_embeddings.shape)\n",
        "print(initial_embeddings)"
      ],
      "id": "6ace1bfa"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Processing Embeddings with the Transformer"
      ],
      "id": "54c8ec00-0531-4ec8-91f9-f91079a61889"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 10, 384])\n",
            "tensor([[[    -0.1642,      0.4660,      0.0489,     -0.4458,     -0.1046,\n",
            "              -0.4053,     -0.0664,     -0.2672,     -0.6509,     -0.0402,\n",
            "           ...,      0.2549,     -0.1511,      0.0672,     -0.3342,\n",
            "               0.0627,      0.1672,     -0.1306,      0.2386,     -0.0663,\n",
            "              -0.0930],\n",
            "         [    -0.5260,      0.8277,      0.7240,      0.0294,      0.4266,\n",
            "              -0.5625,      0.5401,      0.3576,     -1.2588,     -0.1127,\n",
            "           ...,     -0.1882,     -0.3274,     -0.4097,     -0.6994,\n",
            "              -0.2568,      0.0601,     -0.0381,     -0.7451,     -1.1621,\n",
            "               1.1199],\n",
            "         [     0.1473,      0.2505,      0.1579,     -0.7921,      0.1306,\n",
            "              -0.9031,      1.2293,      0.5582,     -0.9292,     -0.2025,\n",
            "           ...,      0.7037,     -0.0821,      0.1968,     -0.4797,\n",
            "              -0.1288,      0.1652,      1.1664,     -0.2087,     -1.1386,\n",
            "              -0.6368],\n",
            "         [    -0.4840,      1.1230,      0.6628,     -0.0099,     -0.5324,\n",
            "              -0.4798,     -0.1430,     -0.3716,     -0.9311,      0.2150,\n",
            "           ...,      0.5581,     -0.7481,      0.5024,      0.0796,\n",
            "              -0.0150,      0.2173,     -0.2530,      0.8590,      0.3046,\n",
            "              -0.1156],\n",
            "         [    -0.5985,      0.2569,      0.3128,     -0.2144,      0.1356,\n",
            "              -0.2739,      0.4818,     -0.4994,     -0.4291,      0.0441,\n",
            "           ...,      0.4164,     -0.0296,      0.0441,     -0.2010,\n",
            "               0.2404,      0.2941,      0.0237,      0.1431,     -0.1129,\n",
            "               0.7390],\n",
            "         [    -0.4394,      0.5312,      0.4112,     -0.3248,     -0.4767,\n",
            "              -0.0145,     -0.8620,      0.1312,     -0.6557,      0.3602,\n",
            "           ...,      0.8515,     -0.1337,      0.8034,     -0.9052,\n",
            "              -0.1578,      0.6671,      0.8386,      0.8715,     -0.3522,\n",
            "               0.3678],\n",
            "         [    -0.9628,      0.4037,      0.0501,     -0.8665,     -0.3116,\n",
            "              -1.8305,     -0.5656,     -0.6143,     -0.8898,     -0.0937,\n",
            "           ...,      0.1572,     -0.5684,     -0.3201,     -0.4960,\n",
            "               0.3975,     -0.2301,     -0.8126,     -0.2742,      0.9110,\n",
            "              -1.1934],\n",
            "         [    -0.5978,      0.6489,     -0.2759,     -0.3145,      0.0450,\n",
            "              -0.4574,      0.4680,     -0.4357,     -0.6042,      0.1676,\n",
            "           ...,      0.3334,      0.2831,      0.1249,     -0.5569,\n",
            "               0.1202,      0.3996,      0.1469,      0.2102,     -0.1217,\n",
            "              -0.2887],\n",
            "         [    -0.8411,      0.7750,      0.2105,     -0.3113,     -0.8924,\n",
            "              -0.1942,      0.7314,      0.1844,     -1.4688,      0.2388,\n",
            "           ...,      1.1335,      0.0039,      0.2320,     -0.4401,\n",
            "               0.2385,      0.1799,      0.1066,      1.0205,     -0.1044,\n",
            "               0.5411],\n",
            "         [    -0.3763,      0.5162,     -0.3255,     -0.7712,      0.0012,\n",
            "              -0.9124,      0.3111,     -0.4361,     -1.1094,     -0.0224,\n",
            "           ...,      0.2102,     -0.3219,      0.0972,     -0.5666,\n",
            "               0.2881,      0.5872,      0.3516,      0.2374,     -0.0547,\n",
            "              -0.1911]]], grad_fn=<NativeLayerNormBackward0>)"
          ]
        }
      ],
      "source": [
        "output = model(**encoded)\n",
        "print(output.last_hidden_state.shape)\n",
        "print(output.last_hidden_state)"
      ],
      "id": "06f7a0bc"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Mean Pooling\n",
        "\n",
        "Since the attention mask consists entirely of `1`s (no padding tokens),\n",
        "we can safely compute the simple mean over all token embeddings. If the\n",
        "attention mask contains `0`s (indicating padding), a weighted mean that\n",
        "accounts for valid tokens is required instead."
      ],
      "id": "6a557a8e-ea6c-4274-b090-3dd46d10de1b"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "assert torch.all(encoded['attention_mask'] == 1)"
      ],
      "id": "c60cdd4e"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.4843,  0.5799,  0.1977, -0.4021, -0.1579, -0.6034,  0.2125, -0.1393,\n",
            "         -0.8927,  0.0554,  ...,  0.4431, -0.2075,  0.1338, -0.4599,  0.0789,\n",
            "          0.2508,  0.1400,  0.2352, -0.1897,  0.0249]],\n",
            "       grad_fn=<MeanBackward1>)"
          ]
        }
      ],
      "source": [
        "pooling = output.last_hidden_state.mean(dim=1)\n",
        "print(pooling)"
      ],
      "id": "31f0805f"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "path": "/usr/share/jupyter/kernels/python3"
    }
  }
}