{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Transformer Sentence Embedding"
      ],
      "id": "0f4f8850-eb70-4630-820c-76fad9bb8168"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoModel, AutoTokenizer\n",
        "import torch"
      ],
      "id": "218b51ff"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_name = 'sentence-transformers/all-MiniLM-L6-v2'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModel.from_pretrained(model_name)\n",
        "\n",
        "text = \"today I solved some excellent algorithmic problems\""
      ],
      "id": "fe70cb28"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## List All Tokens"
      ],
      "id": "754c125c-bc3c-42f1-8f1e-5656d9cfee4d"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token 1: [CLS]\n",
            "Token 2: today\n",
            "Token 3: i\n",
            "Token 4: solved\n",
            "Token 5: some\n",
            "Token 6: excellent\n",
            "Token 7: algorithm\n",
            "Token 8: ##ic\n",
            "Token 9: problems\n",
            "Token 10: [SEP]"
          ]
        }
      ],
      "source": [
        "encoded = tokenizer(text, return_tensors='pt', padding=True, truncation=True)\n",
        "\n",
        "tokens = encoded.input_ids.tolist()[0]\n",
        "\n",
        "for i, token_id in enumerate(tokens):\n",
        "    decoded_token = tokenizer.decode(token_id)\n",
        "    print(f\"Token {i + 1}: {decoded_token}\")"
      ],
      "id": "d265ed25"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Token IDs"
      ],
      "id": "8bf3bd7a-ece0-4008-9ec8-8a5705dc7286"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[  101,  2651,  1045, 13332,  2070,  6581,  9896,  2594,  3471,   102]])\n",
            "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])"
          ]
        }
      ],
      "source": [
        "print(encoded['input_ids'])\n",
        "print(encoded['attention_mask'])"
      ],
      "id": "14b3ce83"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Initial Token Embeddings (Pre-Transformer)"
      ],
      "id": "52f109df-b793-4cf9-b737-811a7225eaec"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 10, 384])\n",
            "tensor([[[-0.1766, -0.0482,  0.0377,  ...,  0.0310,  0.1154, -0.2001],\n",
            "         [ 0.1899,  0.4778, -0.6466,  ...,  0.0499, -0.5361,  0.4469],\n",
            "         [-0.2715, -0.5152,  0.1800,  ..., -0.5518, -0.7233, -0.7180],\n",
            "         ...,\n",
            "         [-0.7215, -0.0884,  0.2669,  ...,  0.9165, -0.4877, -0.6912],\n",
            "         [-0.5559, -0.1996, -0.2737,  ...,  0.8535, -0.2942,  0.7360],\n",
            "         [ 0.2826,  0.1163, -0.2290,  ...,  0.1418, -0.0491, -0.1000]]],\n",
            "       grad_fn=<NativeLayerNormBackward0>)"
          ]
        }
      ],
      "source": [
        "initial_embeddings = model.embeddings(encoded.input_ids)\n",
        "print(initial_embeddings.shape)\n",
        "print(initial_embeddings)"
      ],
      "id": "31505897"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Executing the Model"
      ],
      "id": "11191ca4-e88d-43af-9e04-1374fbff7aa9"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 10, 384])\n",
            "tensor([[[-0.1642,  0.4660,  0.0489,  ...,  0.2386, -0.0663, -0.0930],\n",
            "         [-0.5260,  0.8277,  0.7240,  ..., -0.7451, -1.1621,  1.1199],\n",
            "         [ 0.1473,  0.2505,  0.1579,  ..., -0.2087, -1.1386, -0.6368],\n",
            "         ...,\n",
            "         [-0.5978,  0.6489, -0.2759,  ...,  0.2102, -0.1217, -0.2887],\n",
            "         [-0.8411,  0.7750,  0.2105,  ...,  1.0205, -0.1044,  0.5411],\n",
            "         [-0.3763,  0.5162, -0.3255,  ...,  0.2374, -0.0547, -0.1911]]],\n",
            "       grad_fn=<NativeLayerNormBackward0>)"
          ]
        }
      ],
      "source": [
        "output = model(**encoded)\n",
        "print(output.last_hidden_state.shape)\n",
        "print(output.last_hidden_state)"
      ],
      "id": "29d16dc5"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Mean Pooling\n",
        "\n",
        "Since the attention mask consists entirely of `1`s (no padding tokens),\n",
        "we can safely compute the simple mean over all token embeddings. If the\n",
        "attention mask contains `0`s (indicating padding), a weighted mean that\n",
        "accounts for valid tokens is required instead."
      ],
      "id": "03057210-904c-43a2-8e45-5920e246d500"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "assert torch.all(encoded['attention_mask'] == 1)"
      ],
      "id": "771d0bda"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 384])\n",
            "tensor([[-4.8428e-01,  5.7991e-01,  1.9769e-01, -4.0213e-01, -1.5788e-01,\n",
            "         -6.0335e-01,  2.1247e-01, -1.3929e-01, -8.9269e-01,  5.5423e-02,\n",
            "         -2.8129e-01,  2.2564e-01,  2.0014e-01,  6.2436e-01, -1.8120e-01,\n",
            "          4.4146e-01, -5.3258e-01,  3.2513e-01, -2.6029e-01, -4.1456e-01,\n",
            "         -8.3189e-01,  1.0587e-01, -3.7187e-01,  2.1000e-02, -1.7820e-02,\n",
            "          4.4710e-01,  1.6586e-01, -6.2919e-01,  3.7707e-01, -2.2445e-01,\n",
            "         -1.4096e-01,  2.3643e-01,  2.7513e-01,  3.1801e-02, -1.0134e-01,\n",
            "          1.6924e-01, -1.3359e-01, -6.0268e-02, -1.0087e-01,  1.1035e-01,\n",
            "         -1.0007e-01, -3.9838e-02,  3.1602e-02, -1.0582e-01,  1.3354e-01,\n",
            "         -4.3479e-02, -2.6800e-01,  2.8919e-01,  2.9595e-01, -4.6990e-01,\n",
            "         -6.8446e-01, -3.5652e-01, -4.8612e-01, -3.3313e-01, -1.5210e-01,\n",
            "         -5.2247e-01,  3.6610e-01, -2.6288e-02,  2.5394e-01, -3.9058e-01,\n",
            "          2.1569e-01, -1.2577e-01, -2.3336e-01,  2.8456e-02,  3.2298e-01,\n",
            "          2.5926e-01,  2.8346e-02, -5.5524e-01, -1.3938e-01,  6.1511e-01,\n",
            "         -2.1402e-01,  4.8294e-01, -1.2369e-01,  4.3453e-01,  4.7154e-01,\n",
            "          4.4372e-01,  2.5312e-01, -1.2915e-01, -2.4734e-01, -6.9342e-02,\n",
            "          2.6509e-01, -5.4768e-01, -3.0683e-01,  1.3952e-01,  2.7628e-01,\n",
            "         -3.8644e-01, -4.9635e-01,  2.1763e-01, -4.9969e-04, -3.5792e-01,\n",
            "          3.0984e-01, -1.5872e-01, -1.1209e-01,  3.9097e-02,  4.0962e-01,\n",
            "         -2.7121e-01, -5.0122e-02,  1.3972e-01, -2.0593e-01,  6.5658e-01,\n",
            "         -2.6212e-01,  3.3178e-02, -5.3961e-02, -1.4498e-01,  1.7268e-01,\n",
            "          4.7341e-01,  3.7639e-02,  2.4627e-01,  1.7857e-01, -6.2675e-01,\n",
            "         -1.4247e-01,  1.5706e-01,  2.3983e-01,  1.4251e-01,  1.8673e-01,\n",
            "         -1.0409e-01, -4.5305e-03,  2.8460e-01, -1.3854e-01,  1.0051e-01,\n",
            "         -5.0889e-02,  7.8977e-03,  9.2306e-02,  4.7052e-02,  6.4883e-02,\n",
            "          1.0630e-01, -3.3362e-02, -1.1707e-32, -3.6006e-01,  8.9798e-02,\n",
            "          5.4086e-01, -1.7024e-01,  1.3899e-01, -2.4558e-01, -1.4745e-01,\n",
            "          3.2360e-02, -3.2632e-02, -3.7338e-01,  3.1237e-01,  2.0603e-01,\n",
            "          1.4008e-01, -3.6374e-03,  4.5414e-01, -3.6307e-01,  1.0228e-01,\n",
            "          1.8122e-01, -1.4870e-01, -1.5022e-01,  2.9241e-01, -4.9135e-01,\n",
            "          3.5966e-01, -1.1431e-01,  3.1218e-01,  3.7702e-01,  3.2205e-01,\n",
            "         -8.9393e-01,  5.6245e-01,  7.2465e-02, -1.0439e-01,  5.1047e-02,\n",
            "         -2.8411e-01,  7.9952e-01,  6.7963e-02,  4.6254e-01,  3.6522e-02,\n",
            "         -3.6068e-01,  1.5584e-01, -1.7762e-01,  3.5624e-02,  1.3841e-01,\n",
            "          4.0985e-01, -1.6884e-01,  3.0936e-01, -1.0070e-01, -1.4414e-01,\n",
            "          2.9712e-01,  3.7671e-01,  1.5845e-01, -6.6904e-01,  1.0795e-01,\n",
            "         -1.7819e-01, -2.2329e-01, -8.7882e-02, -7.1969e-02, -3.3885e-01,\n",
            "         -2.3848e-02,  2.3056e-01,  6.1529e-01,  1.0008e-01, -8.6956e-02,\n",
            "          2.2484e-02, -5.0887e-01, -4.4424e-01,  1.9254e-01, -2.7859e-01,\n",
            "         -2.9743e-01,  4.6795e-01,  6.8078e-01, -3.6405e-01,  1.1308e-01,\n",
            "          4.0328e-01,  1.2292e-01,  2.2136e-01,  1.5502e-01,  1.9717e-01,\n",
            "         -4.7296e-01, -2.5361e-01, -3.0272e-01, -9.0734e-03,  3.2617e-01,\n",
            "          1.0640e-01, -6.5876e-01,  4.0050e-01, -3.1732e-01, -1.3504e-01,\n",
            "          3.4852e-02, -2.4832e-01, -9.1921e-02, -4.1656e-01,  3.0411e-01,\n",
            "          2.2005e-01,  1.2296e-01,  2.0818e-01,  1.9593e-33, -4.0086e-01,\n",
            "         -1.3289e-01, -1.9935e-01,  6.9500e-01, -1.6864e-01, -4.7313e-01,\n",
            "         -4.0410e-01, -3.2056e-01,  1.3655e-01,  3.6334e-01,  3.5971e-01,\n",
            "          2.5424e-01, -1.3737e-01,  7.9656e-02, -3.7521e-02, -3.7143e-01,\n",
            "         -2.4727e-01, -3.1417e-01, -3.8764e-02,  9.6059e-02, -4.2674e-01,\n",
            "          7.6613e-01, -3.4031e-01, -1.8983e-01,  2.5721e-01,  2.6458e-01,\n",
            "          2.3309e-01, -5.6291e-03, -4.2669e-02,  3.7605e-01, -1.0608e-01,\n",
            "         -9.8745e-02, -6.7111e-01,  7.5256e-03, -9.0706e-02,  1.6747e-01,\n",
            "          2.9986e-01, -3.2909e-01,  2.8542e-01, -2.8278e-01, -5.4718e-02,\n",
            "          9.7387e-02, -1.8207e-01,  4.2209e-02,  2.2976e-01, -4.9312e-02,\n",
            "         -3.0557e-01,  7.7554e-02,  5.3866e-02,  1.2644e-01, -1.4557e-01,\n",
            "         -5.2327e-02,  1.9259e-01, -4.1808e-01,  1.6583e-01,  2.7267e-01,\n",
            "          9.2375e-03,  1.3124e-01,  2.7824e-01,  1.9643e-01, -7.0674e-01,\n",
            "          2.4170e-01,  4.3383e-02,  8.8427e-02,  2.6052e-01, -4.0119e-01,\n",
            "         -7.2877e-03, -1.1150e-02, -3.4450e-01, -1.4163e-01,  7.0947e-02,\n",
            "          2.5777e-01, -2.7410e-01,  3.7583e-01, -3.9865e-01,  2.1905e-01,\n",
            "          1.8712e-01,  2.2430e-01, -7.7840e-03, -7.1289e-02,  1.7762e-01,\n",
            "          1.5071e-01,  1.3987e-01, -2.2722e-01, -2.6346e-01, -9.8512e-02,\n",
            "          6.3138e-01, -4.1002e-01,  4.4343e-02,  4.6987e-02, -2.3436e-01,\n",
            "          1.2700e-01,  1.9761e-01, -5.4224e-02,  1.5809e-02, -9.5520e-08,\n",
            "         -1.3163e-02, -1.4024e-01,  7.4407e-02,  1.8208e-01,  4.6942e-01,\n",
            "         -1.1864e-01, -2.8480e-01,  4.9853e-01, -6.4665e-01, -4.1281e-01,\n",
            "          4.7916e-01,  1.1282e-01, -6.8950e-02,  6.3927e-01,  2.5271e-01,\n",
            "         -3.9860e-01,  3.9891e-01, -1.8586e-01, -3.5460e-01, -4.1533e-01,\n",
            "          3.2446e-01,  1.9405e-01,  1.2124e-01,  1.4729e-01, -2.5306e-01,\n",
            "          1.9299e-01,  2.8363e-01, -4.6159e-02, -1.6198e-02,  4.3703e-01,\n",
            "          4.2636e-02,  2.3057e-01,  3.5851e-02, -8.3848e-02,  3.5763e-01,\n",
            "          2.5391e-01, -2.3892e-01, -1.3947e-02, -4.6980e-01, -6.8896e-01,\n",
            "          2.0222e-02,  2.4204e-01, -6.3474e-03, -1.8992e-01,  5.2240e-01,\n",
            "          5.3909e-02, -6.8727e-02,  3.3921e-02, -2.2337e-01,  1.8665e-01,\n",
            "         -1.9486e-01,  2.4930e-01,  5.3093e-02,  1.0994e-01,  4.4308e-01,\n",
            "         -2.0752e-01,  1.3381e-01, -4.5994e-01,  7.8891e-02,  2.5076e-01,\n",
            "          1.3996e-01,  2.3524e-01, -1.8974e-01,  2.4910e-02]],\n",
            "       grad_fn=<MeanBackward1>)"
          ]
        }
      ],
      "source": [
        "manual_pooling = output.last_hidden_state.mean(dim=1)\n",
        "print(manual_pooling.shape)\n",
        "print(manual_pooling)"
      ],
      "id": "e6037f5b"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "path": "/usr/share/jupyter/kernels/python3"
    }
  }
}