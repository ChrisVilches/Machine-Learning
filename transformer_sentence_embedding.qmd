---
title: "Transformer Sentence Embedding"
format:
  html:
    code-fold: true
jupyter: python3
---

```{python}
from transformers import AutoModel, AutoTokenizer
import torch
torch.set_printoptions(sci_mode=False)
```

```{python}
model_name = 'sentence-transformers/all-MiniLM-L6-v2'
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModel.from_pretrained(model_name)

text = "today I solved some excellent algorithmic problems"
```

## List All Tokens

```{python}
encoded = tokenizer(text, return_tensors='pt', padding=True, truncation=True)

tokens = encoded.input_ids.tolist()[0]

for i, token_id in enumerate(tokens):
    decoded_token = tokenizer.decode(token_id)
    print(f"Token {i + 1}: {decoded_token}")
```

## Token IDs

```{python}
print(encoded['input_ids'])
print(encoded['attention_mask'])
```

## Initial Token Embeddings (Pre-Transformer)

```{python}
initial_embeddings = model.embeddings(encoded.input_ids)
print(initial_embeddings.shape)
print(initial_embeddings)
```

## Processing Embeddings with the Transformer

```{python}
output = model(**encoded)
print(output.last_hidden_state.shape)
print(output.last_hidden_state)
```

## Mean Pooling

Since the attention mask consists entirely of `1`s (no padding tokens),
we can safely compute the simple mean over all token embeddings.
If the attention mask contains `0`s (indicating padding),
a weighted mean that accounts for valid tokens is required instead.

```{python}
assert torch.all(encoded['attention_mask'] == 1)
```

```{python}
pooling = output.last_hidden_state.mean(dim=1)
print(pooling)
```

